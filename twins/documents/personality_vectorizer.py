# -*- coding: utf-8 -*-
"""personality_vectorizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ygXoxpodak4iaEHyvfK_DuiOOp3ATh-A

Technical Document: Personality Fingerprint Generation and Mobile Deployment
1. Project Goal and Model Overview
The primary objective of this project is to develop a machine learning model that transforms responses from a 100-item personality quiz into "personality fingerprints"â€”trackable, quantifiable vectors that are comparable across individuals. This involves distilling complex questionnaire data into a concise, meaningful numerical representation.

1.1. Core Machine Learning Task: Dimensionality Reduction
Given the goal of creating "trackable, quantifiable vectors" rather than discrete categories, the core machine learning task is dimensionality reduction. This process will convert the 100 individual quiz responses into a smaller, more interpretable set of features that represent an individual's personality profile. This is a form of unsupervised learning, as we are not predicting a predefined label but rather discovering underlying structure in the data.

Suggested Models for Personality Vector Generation:
Factor Analysis (FA):

Type: Unsupervised Learning, Latent Variable Model.

Rationale: FA is a statistical method widely used in psychometrics and personality psychology. It explicitly assumes that observed correlations among quiz items are caused by a smaller number of unobserved "latent factors" or "traits". This aligns perfectly with the theoretical basis of personality models like the Big Five or HEXACO, where traits are considered underlying constructs inferred from responses. The output of FA would be scores on these latent factors, directly forming your personality vector.

Pros: Provides psychologically interpretable factors , directly aligning with established personality theories. Well-suited for uncovering underlying constructs.

Cons: Assumes linear relationships between observed variables and latent factors, which might not always hold true for complex psychological data.

Python Libraries: factor_analyzer, sklearn.decomposition.FactorAnalysis.

Principal Component Analysis (PCA):

Type: Unsupervised Learning, Dimensionality Reduction.

Rationale: PCA transforms a set of potentially correlated variables (your 100 quiz items) into a smaller set of uncorrelated variables called "principal components". It aims to capture as much of the original data's variance (information) as possible within these new components. PCA is a robust method for data compression and noise reduction.

Pros: Simpler to implement and computationally less intensive than Factor Analysis for large datasets. Effective for general data compression and noise reduction.

Cons: While often used interchangeably with FA in software, PCA primarily identifies statistical composites, not necessarily underlying psychological constructs. The components might be less directly interpretable in psychological terms compared to factors from FA.

Python Libraries: sklearn.decomposition.PCA.

Autoencoders (Deep Learning):

Type: Unsupervised Learning, Deep Learning.

Rationale: Autoencoders are neural networks that learn to compress input data into a lower-dimensional "latent space" representation (your personality vector) and then reconstruct the original input. This forces the network to extract the most salient features. Variational Autoencoders (VAEs) are a more advanced variant that can create a more structured and continuous latent space. They are particularly beneficial if the relationships between quiz items and personality traits are complex and non-linear.

Pros: Can capture complex, non-linear relationships in the data. Potentially generate richer, more discriminative "personality fingerprints".

Cons: Interpretability of the latent space can be challenging; understanding the precise psychological meaning of each dimension can be less straightforward than with linear methods. Can struggle with high multicollinearity in input data.

Python Libraries: tensorflow.keras.

Learning Types:
Unsupervised Learning: This is the primary learning type for generating the personality vectors. Models like Factor Analysis, PCA, and Autoencoders fall into this category, as they learn patterns and reduce dimensionality without requiring pre-labeled personality types.

Supervised Learning (Conditional): If, after generating the personality vectors, you wish to classify individuals into discrete personality types (e.g., "Innovator," "Collaborator"), you would then apply supervised classification algorithms. This would require a dataset where personality vectors are explicitly linked to these predefined types. Common algorithms include Support Vector Machines (SVM), Random Forests, Logistic Regression, and various Deep Learning classifiers. However, the initial request focuses on creating

quantifiable vectors, making unsupervised dimensionality reduction the central task.

2. Exploratory Data Analysis (EDA)
Exploratory Data Analysis (EDA) is a crucial initial step to understand the patterns, trends, and relationships within your personality quiz data. This is especially important for Likert scale data.

2.1. Key Steps for EDA:
Import Libraries: Import essential Python libraries like pandas for data manipulation, numpy for numerical operations, and matplotlib.pyplot and seaborn for visualization.

Load Dataset: Read your 100-item personality quiz dataset into a pandas DataFrame.

Initial Data Inspection:

df.shape(): Check the number of rows (observations) and columns (features) to understand the dataset's size and structure.

df.head()/df.tail(): View the first/last few rows to get a sense of the data format.

df.info(): Get a summary of the DataFrame, including data types and non-null values.

df.describe(): Generate descriptive statistics (mean, median, min, max, quartiles) for numerical columns.

Handle Missing Values:

Identify missing values (df.isnull().sum()).

Decide on an imputation strategy (e.g., mean, median, mode imputation) or removal, depending on the extent and nature of missingness.

Check for Duplicate Values: Identify and handle any duplicate entries in the dataset.

Univariate Analysis (Individual Variables):

For Likert Scale Items:

Frequency Distributions: Use bar charts or histograms to visualize the distribution of responses for each quiz item (e.g., how many "Strongly Agree" vs. "Strongly Disagree").

Central Tendency: For individual Likert items, the mode (most frequent response) or median (middle value) are the most appropriate measures of central tendency, as Likert items are ordinal. The mean is generally not recommended for individual Likert items as it assumes equal intervals.

For Aggregated Trait Scores (after Feature Engineering): Once you've aggregated items into trait scores (e.g., Big Five scores), these are often treated as continuous. You can then use histograms, kernel density plots, and box plots to visualize their distribution, variance, and identify outliers.

Bivariate Analysis (Relationships between two variables):

Correlation Matrix/Heatmap: Compute the correlation matrix between quiz items or aggregated trait scores to identify relationships. A heatmap can visually represent these correlations.

Scatter Plots: For numerical trait scores, scatter plots can show relationships between pairs of variables.

Grouped Bar Plots: If comparing Likert responses across demographic groups, grouped bar plots can be useful.

Multivariate Analysis (Relationships among multiple variables):

Pair Plots: Use seaborn.pairplot to visualize the distributions of individual variables and the relationships between pairs of variables in a grid.

Dimensionality Reduction for Visualization: After applying dimensionality reduction (e.g., PCA or t-SNE), visualize the reduced-dimensional data to identify clusters or patterns. t-SNE is particularly good at preserving local structure and revealing clusters in lower dimensions.

2.2. Specific Considerations for Likert Scale Data in EDA:
Ordinal Nature: Remember that individual Likert items are ordinal data, meaning there's a rank order but not necessarily equal intervals between categories.

Numerical Conversion: Convert Likert responses (e.g., "Strongly Disagree" to "Strongly Agree") into numerical values (e.g., 1 to 5) for quantitative analysis. Python's

OrdinalEncoder from scikit-learn or replace() method in pandas can be used.

Aggregation: When multiple Likert items measure a single construct (like a personality trait), summing or averaging these items to create a composite score is a common practice, and these composite scores are often treated as interval data for further analysis.

3. Potential Data Resources
Access to a suitable dataset is critical. Here are some options:

Publicly Available Datasets:

Open Psychometrics Dataset: This dataset contains over a million questionnaire responses with 110 columns, using a 1-5 Likert scale for Big Five traits. This is a strong candidate given your 100-item quiz structure.

IPIP-120: A psychological inventory based on the Big Five model, consisting of 120 items.

IPIP-HEXACO Scales: Offers 240 items on a seven-point Likert scale, designed to reproduce HEXACO-PI scores.

Google Dataset Search: A valuable tool for discovering other relevant datasets.

Kaggle: A platform with numerous datasets, including personality test data.

Custom Data Collection:

If existing datasets don't perfectly match your 100-item quiz, you may need to collect your own data.

Ethical Considerations: Ensure informed consent, data privacy, and anonymity are strictly adhered to. Be mindful of potential biases in data collection (e.g., over-representation of WEIRD populations).

Questionnaire Design: Ensure your 100-item quiz is psychometrically sound, aligning with established personality models (Big Five or HEXACO) to ensure the interpretability and validity of your "personality fingerprints".

4. Roadmap to React Native Mobile App Powered by Google Cloud
This section outlines the practical steps to implement your model and integrate it into a mobile application.

4.1. Phase 1: Model Development and Training (Google Colab)
Set up Google Colab Environment:

Create a new Colab notebook.

Install necessary libraries (e.g., pandas, numpy, scikit-learn, matplotlib, seaborn, tensorflow, factor_analyzer).

Enable GPU/TPU runtime for faster training, especially if using deep learning models.

Data Loading and Preprocessing:

Load your chosen dataset (e.g., from Google Drive, Kaggle, or a public URL).

Implement the EDA steps outlined in Section 2.

Numerical Conversion: Convert Likert scale responses to numerical values (e.g., 1-5) using OrdinalEncoder or replace().

Feature Engineering: Aggregate the 100 individual item responses into trait scores (e.g., 5 Big Five or 6 HEXACO dimensions). This is crucial for creating meaningful "personality fingerprints."

Data Normalization/Scaling: Apply MinMaxScaler to scale your trait scores to a 0-1 range. This is essential for optimal model performance.

Handle Missing Values: Implement imputation strategies as identified during EDA.

Model Selection and Training:

Choose your primary dimensionality reduction model:

Factor Analysis (FA): Use factor_analyzer or sklearn.decomposition.FactorAnalysis to fit the model and extract latent factors. Determine the optimal number of factors (e.g., 5 for Big Five, 6 for HEXACO).

Principal Component Analysis (PCA): Use sklearn.decomposition.PCA to reduce the dimensionality.

Autoencoder: Build a neural network using tensorflow.keras with an encoder and decoder architecture. The bottleneck layer will represent your personality vector. Train the autoencoder to minimize reconstruction loss.

Model Evaluation:

For dimensionality reduction models, evaluate using reconstruction error (how well the original data can be reconstructed from the reduced vector).

For autoencoders, also assess the interpretability of the latent space, though this can be complex.

Model Export for Mobile Deployment:

Once your model is trained and evaluated, convert it to TensorFlow Lite (.tflite) format. This format is optimized for on-device inference, making it fast and efficient for mobile applications.

Use tf.lite.TFLiteConverter.from_keras_model() or from_saved_model() for conversion.

# Task
Read the technical document "technical_document.md" and scaffold a Google Colab notebook based on the instructions provided in the document.

## Set up the environment

### Subtask:
Install necessary libraries and set up the Google Colab environment as described in Phase 1 of the technical document.

**Reasoning**:
The first step is to install the necessary libraries as outlined in the technical document.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install pandas numpy scikit-learn matplotlib seaborn tensorflow factor_analyzer

"""## Load and inspect the dataset

### Subtask:
Load the chosen dataset and perform initial data inspection steps as outlined in Section 2.1 of the technical document.

**Reasoning**:
Load the dataset and perform the initial data inspection steps as outlined in the technical document.
"""

import pandas as pd

# Placeholder for dataset loading.
# Replace this with actual loading code when a dataset is available.
# For demonstration purposes, we will create a dummy DataFrame.
# In a real scenario, you would load your dataset like:
# df = pd.read_csv('your_dataset.csv')

# Create a dummy DataFrame simulating personality quiz data (100 items, 5-point Likert scale)
import numpy as np
np.random.seed(42) # for reproducibility
data = np.random.randint(1, 6, size=(1000, 100))
columns = [f'item_{i+1}' for i in range(100)]
df = pd.DataFrame(data, columns=columns)

# 3. Print the shape of the DataFrame
print("DataFrame Shape:")
print(df.shape)
print("\n")

# 4. Display the first few rows of the DataFrame
print("First 5 Rows:")
display(df.head())
print("\n")

# 5. Print a concise summary of the DataFrame
print("DataFrame Info:")
df.info()
print("\n")

# 6. Generate and display descriptive statistics
print("Descriptive Statistics:")
display(df.describe())

"""## Handle missing and duplicate values

### Subtask:
Implement strategies to handle missing and duplicate values in the dataset as described in Section 2.1.

**Reasoning**:
Check for and handle missing and duplicate values in the DataFrame as instructed in Section 2.1.
"""

# 1. Check for missing values
print("Missing values per column:")
print(df.isnull().sum())
print("\n")

# 2. Handle missing values (if any)
# Based on the previous df.info() output, there are no missing values in this dummy df.
# If there were missing values, we would implement an imputation strategy here.
# Example (using mean imputation, though median/mode might be better for Likert):
# for col in df.columns:
#     if df[col].isnull().any():
#         df[col].fillna(df[col].mean(), inplace=True)
# print("Missing values after handling:")
# print(df.isnull().sum()) # Verify missing values are gone
# print("\n")

# 3. Check for duplicate rows
print("Number of duplicate rows:")
print(df.duplicated().sum())
print("\n")

# 4. Handle duplicate rows (if any)
# Based on the previous df.duplicated().sum() output, there are no duplicate rows in this dummy df.
# If there were duplicate rows, we would remove them here.
# Example:
# if df.duplicated().sum() > 0:
#     df.drop_duplicates(inplace=True)
#     print("DataFrame shape after removing duplicates:")
#     print(df.shape)
# else:
#     print("No duplicate rows found.")

"""## Perform univariate analysis

### Subtask:
Conduct univariate analysis on individual variables, considering the specific considerations for Likert scale data mentioned in Section 2.2.

**Reasoning**:
Iterate through each item column, calculate and print the frequency distribution, mode, and median for each, and then visualize the distribution for a subset of columns using bar plots.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Set plotting style
sns.set_style("whitegrid")

# 1. Iterate through each item column and perform univariate analysis
print("Univariate Analysis for Likert Scale Items:")
for col in df.columns:
    print(f"\n--- Analysis for {col} ---")

    # 2. Calculate and print the frequency distribution
    freq_dist = df[col].value_counts().sort_index()
    print("Frequency Distribution:")
    print(freq_dist)

    # 3. Calculate and print the mode
    # Use .mode()[0] to handle cases with multiple modes by taking the first one
    mode_val = df[col].mode()[0]
    print(f"Mode: {mode_val}")

    # 4. Calculate and print the median response
    median_val = df[col].median()
    print(f"Median: {median_val}")

# 5. Generate and display bar plots for a representative subset of item columns
print("\nVisualizing Frequency Distributions for a Subset of Items:")

# Select a subset of columns to visualize (e.g., first 10 items)
subset_cols = df.columns[:10]
n_cols = len(subset_cols)

# Determine grid size for subplots
n_rows = (n_cols + 3) // 4 # Arrange in rows of 4 plots
fig, axes = plt.subplots(n_rows, 4, figsize=(16, n_rows * 4))
axes = axes.flatten() # Flatten the array of axes for easy iteration

for i, col in enumerate(subset_cols):
    ax = axes[i]
    sns.countplot(x=col, data=df, ax=ax, palette="viridis")
    ax.set_title(f'Distribution of {col}')
    ax.set_xlabel("Response")
    ax.set_ylabel("Count")
    # Ensure integer ticks for Likert scale responses
    ax.set_xticks(sorted(df[col].unique()))

# Hide any unused subplots
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""## Perform bivariate and multivariate analysis

### Subtask:
Perform bivariate and multivariate analysis to explore relationships between variables as described in Section 2.1.

**Reasoning**:
Perform bivariate and multivariate analysis using correlation matrix heatmap and pair plots on a subset of columns as described in the instructions.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# 1. Calculate the correlation matrix
correlation_matrix = df.corr()

# 2. Generate a heatmap of the correlation matrix
print("Correlation Matrix Heatmap:")
plt.figure(figsize=(20, 18)) # Adjust figure size for better visibility
sns.heatmap(correlation_matrix, cmap='coolwarm', annot=False) # annot=False for large matrix
plt.title('Correlation Matrix of Quiz Items')
plt.show()

# 3. Use seaborn.pairplot on a small subset of columns (e.g., the first 10 columns)
print("\nPair Plot for a Subset of Quiz Items (First 10 columns):")
subset_df = df.iloc[:, :10] # Select the first 10 columns
sns.pairplot(subset_df, diag_kind='kde') # Use kde for diagonal for numerical data
plt.suptitle('Pairwise Relationships and Distributions for First 10 Quiz Items', y=1.02) # Add a title
plt.show()

"""## Convert likert scale data

### Subtask:
Convert Likert scale responses to numerical values as discussed in Section 2.2.

**Reasoning**:
Confirming that the data types are numerical and verifying the existing format of the data based on the instructions and previous inspection steps.
"""

# 1. Confirm that the data in the DataFrame df is already in numerical format (e.g., 1-5)
# Based on previous steps (df.info() and df.head()), the data is already in numerical format (int64).
print("Based on previous inspection (df.info() and df.head()), the data is already in numerical format.")

# 2. If the data were in string format, use replace() or OrdinalEncoder.
# Since the dummy data is already numerical, this step is a confirmation.
# If df['item_1'].dtype == 'object': # Example check for string data type
    # Assuming a mapping from strings to numbers
    # likert_mapping = {"Strongly Disagree": 1, "Disagree": 2, "Neutral": 3, "Agree": 4, "Strongly Agree": 5}
    # df.replace(likert_mapping, inplace=True)
    # Or using OrdinalEncoder
    # from sklearn.preprocessing import OrdinalEncoder
    # encoder = OrdinalEncoder(categories=[list(likert_mapping.keys())])
    # df[df.columns] = encoder.fit_transform(df[df.columns]) + 1 # Add 1 if encoding starts from 0

print("The dummy data is already in numerical format, so no conversion from string was needed.")

# 3. Print the data types of the DataFrame columns to verify they are numerical
print("\nData types of DataFrame columns:")
print(df.dtypes)

# Further verification by checking the data type of a sample column
print(f"\nData type of 'item_1': {df['item_1'].dtype}")

"""## Perform feature engineering

### Subtask:
Aggregate individual item responses into trait scores as described in Section 4.1.

**Reasoning**:
The next step is to aggregate the individual item responses into trait scores by defining a mapping and calculating the average for each trait, as per the instructions. Then, display the first few rows and descriptive statistics of the updated DataFrame to verify the aggregation.
"""

# 1. Define a plausible mapping of items to personality traits (simplified Big Five)
# For demonstration, let's assume items 1-20 map to Trait A, 21-40 to Trait B, etc.
# In a real scenario, this mapping would come from the quiz design documentation.
trait_mapping = {
    'Trait_A': [f'item_{i}' for i in range(1, 21)],
    'Trait_B': [f'item_{i}' for i in range(21, 41)],
    'Trait_C': [f'item_{i}' for i in range(41, 61)],
    'Trait_D': [f'item_{i}' for i in range(61, 81)],
    'Trait_E': [f'item_{i}' for i in range(81, 101)]
}

# 2. Create new columns for each personality trait and calculate the trait score
for trait, items in trait_mapping.items():
    # Ensure all items in the mapping exist in the DataFrame columns
    existing_items = [item for item in items if item in df.columns]
    if existing_items:
        df[trait] = df[existing_items].mean(axis=1)
    else:
        print(f"Warning: None of the items for {trait} found in DataFrame columns.")


# 4. Display the first few rows of the DataFrame with the new trait score columns
print("DataFrame with Trait Scores:")
display(df.head())

# 5. Display descriptive statistics for the new trait score columns
print("\nDescriptive Statistics for Trait Scores:")
display(df[['Trait_A', 'Trait_B', 'Trait_C', 'Trait_D', 'Trait_E']].describe())

"""## Normalize/scale data

### Subtask:
Apply normalization or scaling to the data as suggested in Section 4.1.

**Reasoning**:
Apply MinMaxScaler to the trait score columns and display the head and descriptive statistics of the DataFrame with the scaled values.
"""

from sklearn.preprocessing import MinMaxScaler

# 2. Instantiate a MinMaxScaler object.
scaler = MinMaxScaler()

# 3. Select the trait score columns from the DataFrame df to be scaled.
trait_cols = ['Trait_A', 'Trait_B', 'Trait_C', 'Trait_D', 'Trait_E']
df_traits = df[trait_cols]

# 4. Fit the scaler to the selected trait score columns and transform them.
df_scaled_traits = scaler.fit_transform(df_traits)

# 5. Create a new DataFrame or replace the existing trait score columns in df with the scaled values.
# Replace the original trait columns with the scaled values
df[trait_cols] = df_scaled_traits

# 6. Display the head of the DataFrame with the scaled trait scores and their descriptive statistics to verify the scaling.
print("DataFrame with Scaled Trait Scores:")
display(df.head())

print("\nDescriptive Statistics for Scaled Trait Scores:")
display(df[trait_cols].describe())

"""## Model selection and training

### Subtask:
Generate code for selecting and training a model as described in Section 4.1.

**Reasoning**:
Generate code for selecting and training a PCA model on the scaled trait data, transforming the data, and storing the result.
"""

from sklearn.decomposition import PCA

# 1. Select the scaled trait columns ('Trait_A', 'Trait_B', 'Trait_C', 'Trait_D', 'Trait_E')
# This step was already done in the previous subtask and stored within the 'df' DataFrame
# with columns 'Trait_A' through 'Trait_E' containing the scaled values.
X = df[['Trait_A', 'Trait_B', 'Trait_C', 'Trait_D', 'Trait_E']]
print("Selected data for PCA:")
display(X.head())

# 3. Import the chosen model class (already done above)

# 4. Instantiate the PCA model with 3 components.
pca = PCA(n_components=3)
print(f"\nInstantiated PCA model with {pca.n_components} components.")

# 5. Fit the model to the scaled trait data using the .fit() method.
pca.fit(X)
print("PCA model fitted to the data.")

# 6. Transform the scaled trait data using the fitted model.
X_pca = pca.transform(X)
print("Data transformed using PCA.")

# 7. Store the transformed data in a new DataFrame.
df_pca = pd.DataFrame(X_pca, columns=[f'PCA_Component_{i+1}' for i in range(X_pca.shape[1])])
print("\nTransformed data stored in a new DataFrame:")
display(df_pca.head())

# Optional: Display the explained variance ratio of the components
print("\nExplained variance ratio of each component:")
print(pca.explained_variance_ratio_)
print(f"Total explained variance: {pca.explained_variance_ratio_.sum():.4f}")

"""## Model evaluation

### Subtask:
Generate code for evaluating the trained model as described in Section 4.1.

**Reasoning**:
Calculate the reconstruction error of the PCA model by inverse transforming the PCA data and comparing it to the original data using Mean Squared Error.
"""

from sklearn.metrics import mean_squared_error

# 1a. Inverse transform the PCA-transformed data back to the original scaled trait space.
X_reconstructed = pca.inverse_transform(df_pca)

# Convert the reconstructed data back to a DataFrame for easier comparison (optional but good practice)
X_reconstructed_df = pd.DataFrame(X_reconstructed, columns=X.columns)
print("Reconstructed data (first 5 rows):")
display(X_reconstructed_df.head())

# 1b. Calculate the Mean Squared Error (MSE) between the original scaled trait data (X) and the inverse-transformed data.
# Ensure both DataFrames have the same index and columns before calculating MSE.
# Since X was derived directly from df and df_pca was created from X_pca which was transformed from X,
# the row order should match. Column names are explicitly set for X_reconstructed_df.
reconstruction_error = mean_squared_error(X, X_reconstructed_df)

# 2. Print the calculated reconstruction error.
print(f"\nMean Squared Error (MSE) Reconstruction Error: {reconstruction_error:.6f}")

"""**Reasoning**:
The traceback indicates that the `X_reconstructed_df` DataFrame contains NaN values, which caused the `mean_squared_error` function to fail. This is likely due to an issue during the creation or display of `X_reconstructed_df`. The original `X_reconstructed` numpy array should not have NaNs if the input `df_pca` is finite, and `df_pca` is derived from the scaled input `X` which was checked for NaNs in previous steps. Let's re-examine the reconstructed data and ensure it's correctly converted to a DataFrame without introducing NaNs. We can directly calculate the MSE using the numpy arrays `X.values` and `X_reconstructed` to avoid potential issues with the DataFrame conversion and indexing shown in the previous attempt's display output.


"""

from sklearn.metrics import mean_squared_error
import numpy as np

# 1a. Inverse transform the PCA-transformed data back to the original scaled trait space.
# df_pca contains the PCA transformed data
X_reconstructed = pca.inverse_transform(df_pca)

# Verify the reconstructed data is not NaN before calculating MSE
print(f"Are there any NaNs in the reconstructed data? {np.isnan(X_reconstructed).any()}")


# 1b. Calculate the Mean Squared Error (MSE) between the original scaled trait data (X) and the inverse-transformed data.
# Use the underlying numpy arrays for the calculation to avoid potential DataFrame indexing issues.
reconstruction_error = mean_squared_error(X.values, X_reconstructed)

# 2. Print the calculated reconstruction error.
print(f"\nMean Squared Error (MSE) Reconstruction Error: {reconstruction_error:.6f}")

"""## Model export

### Subtask:
Generate code to export the model in a format suitable for mobile deployment as described in Section 4.1.

**Reasoning**:
Generate code to export the trained PCA model to TensorFlow Lite format for mobile deployment.
"""

import tensorflow as tf

# 1. Define the input shape for the model.
# This is the number of features the PCA model was trained on, which is the number of scaled trait scores.
input_shape = [X.shape[1]]
print(f"Input shape for the model: {input_shape}")

# 2. Create a simple sequential Keras model with a lambda layer.
# This lambda layer will apply the scikit-learn PCA transformation.
# We need to define the PCA transformation logic within the lambda layer.
# Note: This approach essentially wraps the sklearn PCA logic in a Keras layer.
# For TFLite compatibility, complex sklearn objects within Lambda might be problematic.
# A more robust approach for TFLite would be to implement the PCA matrix multiplication
# and mean subtraction directly using TensorFlow operations within the Lambda layer,
# using the fitted 'mean_' and 'components_' attributes of the pca object.

# Let's implement the PCA transformation using TensorFlow operations for TFLite compatibility.
# Get the mean and components from the fitted sklearn PCA model
pca_mean = pca.mean_
pca_components = pca.components_

# Convert numpy arrays to TensorFlow constants
pca_mean_tf = tf.constant(pca_mean, dtype=tf.float32)
pca_components_tf = tf.constant(pca_components, dtype=tf.float32)

# Define the Keras model
model = tf.keras.Sequential([
    # Input layer matching the shape of the scaled trait data
    tf.keras.Input(shape=input_shape, name='input_traits'),
    # Lambda layer to perform the PCA transformation using TensorFlow ops
    tf.keras.layers.Lambda(
        lambda x: tf.matmul(x - pca_mean_tf, tf.transpose(pca_components_tf)),
        name='pca_transform_layer'
    )
])

print("\nKeras model created for TFLite conversion.")
model.summary()

# 3. Convert the Keras model to a TensorFlow Lite model.
converter = tf.lite.TFLiteConverter.from_keras_model(model)
# Optionally, optimize the model for size or performance
# converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()
print("\nKeras model converted to TensorFlow Lite format.")

# 4. Save the TensorFlow Lite model to a file.
tflite_file_path = 'pca_model.tflite'
with open(tflite_file_path, 'wb') as f:
    f.write(tflite_model)

# 5. Print a confirmation message.
print(f"\nTensorFlow Lite model saved successfully to '{tflite_file_path}'")

"""## Summary:

### Data Analysis Key Findings

*   The necessary libraries (`pandas`, `numpy`, `scikit-learn`, `matplotlib`, `seaborn`, `tensorflow`, `factor_analyzer`) were successfully installed in the Google Colab environment.
*   A dummy dataset simulating 1000 responses to 100 Likert scale items was created and successfully loaded into a pandas DataFrame.
*   Initial data inspection confirmed the DataFrame's shape (1000, 100), data types (all `int64`), and the absence of missing or duplicate values in the dummy data.
*   Univariate analysis was performed on each item, calculating and displaying frequency distributions, mode, and median. Bar plots for a subset of items visualized these distributions.
*   Bivariate and multivariate analysis included calculating and visualizing the correlation matrix of all items using a heatmap and generating a pair plot for a subset of items to explore pairwise relationships.
*   The Likert scale data was confirmed to be already in numerical format (`int64`), so no explicit conversion step was needed.
*   Feature engineering successfully aggregated individual item responses into 5 personality trait scores by averaging relevant item responses, adding these as new columns to the DataFrame.
*   The calculated trait scores were successfully scaled using `MinMaxScaler`, resulting in values between 0 and 1.
*   A PCA model with 3 components was successfully trained on the scaled trait scores.
*   The PCA model's reconstruction error (MSE) was calculated as 0.009418, indicating the loss of information when reducing dimensionality to 3 components.
*   The trained PCA transformation logic was successfully wrapped in a Keras model using TensorFlow operations and exported to a TensorFlow Lite file named 'pca\_model.tflite', suitable for mobile deployment.

### Insights or Next Steps

*   The low reconstruction error suggests that the 3 PCA components capture a significant portion of the variance in the 5 trait scores, making the dimensionality reduction effective for potential downstream tasks or visualization.
*   The exported TFLite model provides a lightweight format for deploying the personality trait scoring and dimensionality reduction logic on mobile or edge devices, enabling local processing of quiz responses.

# Task
Create an interactive quiz based on the questions in "/content/Administering IPIP Measures, with a 50-item Sample Questionnaire.html", allow the user to input their answers, and then process these answers to generate and display a 3-component personality fingerprint using the previously trained PCA model and scaler.

The Data Source for this is from: https://ipip.ori.org/new_ipip-50-item-scale.htm

# Task
Create an interactive quiz form based on the questions in the file "/content/ipip_personality_questionnaire.csv", process the user's input, calculate personality trait scores, scale them, generate a 3-component personality fingerprint using PCA, and display the fingerprint.

## Load and extract questions

### Subtask:
Load the CSV file containing the quiz questions and extract the questions.

**Reasoning**:
Load the CSV file and extract the questions as instructed.
"""

import pandas as pd

# Load the CSV file into a pandas DataFrame
quiz_df = pd.read_csv("/content/ipip_personality_questionnaire.csv")

# Extract the 'Question' column
quiz_questions = quiz_df['Question']

# Print the first few questions to verify
print("First 5 Quiz Questions:")
print(quiz_questions.head())

"""## Define trait mapping

### Subtask:
Define a plausible mapping of the items to personality traits for feature engineering.

**Reasoning**:
Define the user_trait_mapping dictionary based on the quiz_df DataFrame to group items by personality trait and then print the mapping to verify.
"""

# 1. Define a dictionary named user_trait_mapping
user_trait_mapping = {}

# Iterate through the unique factor names in the quiz_df
for factor_name in quiz_df['Factor_Name'].unique():
    # Filter the DataFrame for the current factor name
    factor_items_df = quiz_df[quiz_df['Factor_Name'] == factor_name]
    # Extract the item numbers and convert them to a list of strings
    # Assuming item numbers in the main dataframe are 'item_1', 'item_2', etc.
    item_numbers = [f"item_{item}" for item in factor_items_df['Item_Number'].tolist()]
    # Add the factor name and its associated item numbers to the dictionary
    user_trait_mapping[factor_name] = item_numbers

# 2. Print the user_trait_mapping dictionary to verify the mapping.
print("User Trait Mapping:")
print(user_trait_mapping)

"""## Create interactive quiz form

### Subtask:
Generate a code cell with an interactive form to collect user responses for the questions.

**Reasoning**:
Generate a code cell with an interactive form to collect user responses for the questions using ipywidgets, storing the responses in a dictionary keyed by item number.
"""

import ipywidgets as widgets
from IPython.display import display

# Create an empty dictionary to store user responses, keyed by item number string
user_responses = {}

# Define options for the Likert scale responses
likert_options = ["Strongly Disagree", "Disagree", "Neutral", "Agree", "Strongly Agree"]

# Iterate through the quiz questions and create interactive widgets
for index, question in quiz_questions.items():
    # Get the item number (assuming 1-based indexing from the CSV)
    item_number = index + 1
    item_key = f'item_{item_number}'

    # Create a Label widget for the question
    question_label = widgets.Label(f"Item {item_number}: {question}")

    # Create RadioButtons for the Likert scale responses
    response_buttons = widgets.RadioButtons(
        options=likert_options,
        description='Your response:',
        disabled=False,
        layout=widgets.Layout(width='auto') # Adjust width as needed
    )

    # Use interactive_output to capture the selected value
    # The update_response function will store the selected value
    def update_response(change, item_key=item_key):
        user_responses[item_key] = change['new']

    # Observe changes in the RadioButtons value and call update_response
    response_buttons.observe(update_response, names='value')

    # Display the question label and the radio buttons
    display(question_label)
    display(response_buttons)

# Note: The user responses will be stored in the user_responses dictionary
# as the user interacts with the radio buttons.

"""**Reasoning**:
Convert the collected user responses from the Likert scale strings to numerical values (1-5) and store them in a pandas DataFrame, aligning with the format used for the model training. Then display the first few rows and the data types of the resulting DataFrame to verify the conversion.


"""

import pandas as pd

# 1. Define the mapping from Likert scale strings to numerical values (1-5).
# Based on the order of options in the previous cell: ["Strongly Disagree", "Disagree", "Neutral", "Agree", "Strongly Agree"]
likert_to_numerical = {
    "Strongly Disagree": 1,
    "Disagree": 2,
    "Neutral": 3,
    "Agree": 4,
    "Strongly Agree": 5
}

# 2. Create a list of item keys based on the quiz questions.
item_keys = [f'item_{i+1}' for i in range(len(quiz_questions))]

# 3. Convert the collected user_responses (string values) to numerical values using the mapping.
# Create a new dictionary to store numerical responses.
numerical_responses = {}
for item_key in item_keys:
    # Get the string response from user_responses. If a response is missing,
    # use a default value (e.g., Neutral=3) or handle appropriately based on requirements.
    # For this example, we'll assume all questions were answered.
    string_response = user_responses.get(item_key, "Neutral") # Default to Neutral if somehow missing

    # Convert the string response to its numerical equivalent.
    numerical_value = likert_to_numerical.get(string_response, 3) # Default to 3 if string is unexpected

    # Store the numerical value.
    numerical_responses[item_key] = numerical_value


# 4. Create a pandas DataFrame from the numerical responses.
# Reshape the dictionary into a format suitable for a DataFrame (one row per user).
user_df = pd.DataFrame([numerical_responses])

# Ensure the columns are in the same order as the training data if necessary,
# though the trait mapping will handle this later.

# 5. Display the first few rows of the resulting DataFrame to verify.
print("DataFrame of User Responses (Numerical):")
display(user_df.head())

# 6. Print the data types of the DataFrame columns to confirm they are numerical.
print("\nData types of User Responses DataFrame:")
print(user_df.dtypes)

"""**Reasoning**:
Aggregate the numerical user responses into personality trait scores using the defined mapping (`user_trait_mapping`), scale these trait scores using the previously fitted scaler (`scaler`), and then apply the previously trained PCA model (`pca`) to generate the 3-component personality fingerprint. Display the resulting fingerprint.


"""

import pandas as pd

# 1. Aggregate individual item responses into trait scores for the user responses.
user_trait_scores = {}
for trait, items in user_trait_mapping.items():
    # Ensure all items in the mapping exist in the user_df columns
    existing_items = [item for item in items if item in user_df.columns]
    if existing_items:
        # Calculate the mean score for the trait from the user's responses
        user_trait_scores[trait] = user_df[existing_items].mean(axis=1).iloc[0]
    else:
        print(f"Warning: None of the items for {trait} found in user responses.")

# Convert the user trait scores dictionary to a DataFrame (single row)
user_trait_df = pd.DataFrame([user_trait_scores])

# 2. Scale the user trait scores using the previously fitted scaler.
# The scaler was fitted on the training data, so we just use transform here.
# Ensure the order of columns matches the order the scaler was fitted on if necessary.
# Based on previous code, the scaler was fitted on ['Trait_A', 'Trait_B', 'Trait_C', 'Trait_D', 'Trait_E'].
# We need to align the user_trait_df columns to this order.
scaled_trait_cols_order = ['Extraversion', 'Agreeableness', 'Conscientiousness', 'Emotional Stability', 'Intellect/Imagination']

# Reindex user_trait_df to match the order of the scaler's training data columns
# Assuming the user_trait_mapping keys correspond to the traits the scaler was trained on,
# but the key names might differ (e.g., 'Trait_A' vs 'Extraversion').
# We need to ensure the correct mapping between user_trait_mapping keys and scaled_trait_cols_order.
# Let's assume the keys in user_trait_mapping directly correspond to the order
# used for the original df_traits which was scaled. If not, a proper mapping is needed.
# Based on the previous notebook cells, the scaler was fitted on 'Trait_A', 'Trait_B', etc.
# We need to apply the scaling to the user_trait_df columns that correspond to these.
# Assuming a direct correspondence based on the order of creation in the aggregation step.
# Let's explicitly map the user_trait_mapping keys to the original trait names for clarity.
# Based on the previous code, the original trait_mapping was {Trait_A: item_1-20, ...}.
# The user_trait_mapping is {Extraversion: item_1,6,11..., Agreeableness: item_2,7,12...}.
# So, Extraversion corresponds to Trait_A, Agreeableness to Trait_B, etc.

# Create a DataFrame with the same column names and order as the data used for fitting the scaler
# using the calculated user trait scores.
# Map user_trait_mapping keys to original trait names used for scaling
user_trait_scores_mapped = {
    'Trait_A': user_trait_scores['Extraversion'],
    'Trait_B': user_trait_scores['Agreeableness'],
    'Trait_C': user_trait_scores['Conscientiousness'],
    'Trait_D': user_trait_scores['Emotional Stability'],
    'Trait_E': user_trait_scores['Intellect/Imagination']
}

user_trait_df_aligned = pd.DataFrame([user_trait_scores_mapped])

# Apply the scaler
user_scaled_traits = scaler.transform(user_trait_df_aligned)

# Convert scaled results back to a DataFrame with meaningful column names
user_scaled_trait_df = pd.DataFrame(user_scaled_traits, columns=['Scaled_Trait_A', 'Scaled_Trait_B', 'Scaled_Trait_C', 'Scaled_Trait_D', 'Scaled_Trait_E'])
print("User Scaled Trait Scores:")
display(user_scaled_trait_df)


# 3. Apply the trained PCA model to the scaled user trait scores.
user_pca_fingerprint = pca.transform(user_scaled_traits)

# 4. Store the personality fingerprint in a DataFrame.
user_fingerprint_df = pd.DataFrame(user_pca_fingerprint, columns=[f'Personality_Component_{i+1}' for i in range(user_pca_fingerprint.shape[1])])

# 5. Display the personality fingerprint.
print("\nUser Personality Fingerprint (3 PCA Components):")
display(user_fingerprint_df)

# The subtask is completed.

"""## Summary:

### Data Analysis Key Findings

*   An interactive quiz form was successfully created using `ipywidgets` to collect user responses to the personality questionnaire items.
*   User responses were captured as strings from the Likert scale radio buttons.
*   The string responses were converted into numerical values (1-5) and stored in a pandas DataFrame.
*   The numerical item responses were aggregated to calculate scores for 5 personality traits.
*   These personality trait scores were scaled using a pre-fitted scaler.
*   A 3-component personality fingerprint was generated by applying a pre-trained PCA model to the scaled trait scores.

### Insights or Next Steps

*   Visualize the generated personality fingerprint to provide a graphical representation to the user.
*   Compare the user's personality fingerprint to a larger dataset of fingerprints to provide context and potential insights into their personality relative to others.

"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Assuming user_scaled_trait_df contains the scaled trait scores from the previous step
# The columns should be in the order of the traits: Extraversion, Agreeableness, Conscientiousness, Emotional Stability, Intellect/Imagination
# Let's use the keys from user_trait_mapping for the labels, assuming the order is consistent

# Get the trait labels from the user_trait_mapping keys
labels = list(user_trait_mapping.keys())

# Get the scaled trait values for the user
# Ensure the order of values matches the order of labels
user_scaled_values = user_scaled_trait_df.iloc[0].values

# Number of variables we're plotting.
num_vars = len(labels)

# Create a list of angles for each axis in the plot.
angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()

# The plot is circular, so we need to "close the loop"
# by repeating the first value and the first angle.
user_scaled_values = np.concatenate((user_scaled_values, [user_scaled_values[0]]))
angles = angles + [angles[0]]

# Create the polar plot
fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))

# Plot the user's profile
ax.plot(angles, user_scaled_values, linewidth=2, linestyle='solid', label='Your Profile')
# Fill the area under the line
ax.fill(angles, user_scaled_values, 'blue', alpha=0.25)

# Set the angle for each axis and the labels
ax.set_theta_offset(np.pi / 2) # Rotate the plot so the first axis is at the top
ax.set_theta_direction(-1)    # Go clockwise

# Draw axis lines and labels
ax.set_xticks(angles[:-1]) # Set ticks at the angles (excluding the last repeated one)
ax.set_xticklabels(labels) # Set the labels for each axis

# Set the y-axis limits and labels (0 to 1 since the data is scaled)
ax.set_ylim(0, 1)
ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
ax.set_yticklabels(["0.2", "0.4", "0.6", "0.8", "1.0"])

# Add a title
plt.title('Your Personality Profile (Scaled Trait Scores)', size=20, color='grey', y=1.1)

# Add a legend
ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))

# Display the plot
plt.show()